♦ Il existe 2 types de parcours des valeurs d’un arbre : parcours en profondeur et
parcours en largeur.
1. Parcours en profondeur
♦ Pour ce type de parcours; on distingue entre 3 algorithmes récursifs simple permettent le
parcours en profondeur d’un arbre binaire : préfixé, infixé et postfixé.
♦ Tous les nœuds de l’arbre sont atteints branche par branche dans toute leur profondeur.

N°1

N°2 N°6

N°3

N°4 N°5

N N°8 °7

N°5

N°4 N°7

N°2

N°1 N°3

N N°8 °6

N°8

N°4 N°7

N°3

N°1 N°2

N N°6 °5

Parcours préfixé Parcours infixé Parcours suffixé ou post fixé
Pr. B.HSSINA badr.hssina@univh2c.ma Module I133: Structures de données 198

6. Les parcours d’arbre
parcours préfixé : le traitement de la racine se fait avant la visite des sous
arbres
void parcoursPrefixe(Arbre *ar) {

printf("%d ", ar->valeur); //traitement racine
if (ar->sag !=NULL) parcoursPrefixe(ar->sag); // appel récursive sag
if (ar->sad !=NULL) parcoursPrefixe(ar->sad); // appel récursive sad

}

Le parcours préfixé donne : 5 9 7 4 2 3

5

9 2

7 4 3

Pr. B.HSSINA badr.hssina@univh2c.ma Module I133: Structures de données 199

6. Les parcours d’arbre
parcours infixé : le traitement de la racine se fait entre les deux visites des
sous arbres
void parcoursInfixe(Arbre *ar) {

if (ar->sag !=NULL) parcoursInfixe(ar->sag); // appel récursive sag
printf("%d ", ar->valeur); //traitement racine
if (ar->sad !=NULL) parcoursInfixe(ar->sad); // appel récursive sad

}

Le parcours infixé donne : 7 9 4 5 2 3

5

9 2

7 4 3

Pr. B.HSSINA badr.hssina@univh2c.ma Module I133: Structures de données 200

6. Les parcours d’arbre
parcours postfixé (suffixé) : le traitement de la racine se fait après la visite
des sous arbres
void parcoursPostfixe(Arbre *ar) {

if (ar->sag !=NULL) parcoursPostfixe(ar->sag); // appel récursive sag
if (ar->sad !=NULL) parcoursPostfixe(ar->sad); // appel récursive sad
printf("%d ", ar->valeur); //traitement racine

}

Le parcours postfixé donne : 7 4 9 3 2 5

5

9 2

7 4 3

Pr. B.HSSINA badr.hssina@univh2c.ma Module I133: Structures de données 201

6. Les parcours d’arbre
2. Parcours en largeur
♦ Tous les nœuds de l’arbre sont atteints depuis la racine, puis couche par
couche de gauche à droit.
♦ Pour écrire cette méthode, il faut introduire une liste d’arbres où seront
stockés les nœuds au fur et à mesure de leur passage.
♦ Il faut ajouter en tête et retirer en queue : une file (FIFO) aurait même
suffit.
Once an error is detected, how should the parser recover? Although no strategy 
has proven itself universally acceptable, a few methods have broad applicabil- 
ity. The simplest approach is for the parser to quit with an informative error 
message when it detects the first error. Additional errors are often uncovered 
if 
the parser can restore itself to a state where processing of the input can con- 
tinue with reasonable hopes that the further processing will provide meaningful 
diagnostic information. If errors pile up, it is better for the compiler to give 
up after exceeding some error limit than to produce an annoying avalanche of 
"spurious" errors. 
The balance of this section is devoted to the following recovery strategies: 
panic-mode, phrase-level, error-productions, and global-correction. 
Panic-Mode Recovery 
With this method, on discovering an error, the parser discards input symbols 
one at a time until one of a designated set of synchronizing tokens is found. 
The synchronizing tokens are usually delimiters, such as semicolon or 3, whose 
role in the source program is clear and unambiguous. The compiler designer 
CHAPTER 4. SYNTAX ANALYSIS 
must select the synchronizing tokens appropriate for the source language. While 
panic-mode correction often skips a considerable amount of input without check- 
ing it for additional errors, it has the advantage of simplicity, and, unlike some 
methods to be considered later, is guaranteed not to go into an infinite loop. 
Phrase-Level Recovery 
On discovering an error, a parser may perform local correction on the remaining 
input; that is, it may replace a prefix of the remaining input by some string that 
allows the parser to continue. A typical local correction is to replace a comma 
by a semicolon, delete an extraneous semicolon, or insert a missing semicolon. 
The choice of the local correction is left to the compiler designer. Of course, 
we must be careful to choose replacements that do not lead to infinite loops, as 
would be the case, for example, if we always inserted something on the input 
ahead of the current input symbol. 
Phrase-level replacement has been used in several error-repairing compilers, 
as it can correct any input string. Its major drawback is the difficulty it has in 
coping with situations in which the actual error has occurred before the point 
of detection. 
Error Product ions 
By anticipating common errors that might be encountered, we can augment the 
grammar for the language at hand with productions that generate the erroneous 
constructs. A parser constructed from a grammar augmented by these error 
productions detects the anticipated errors when an error production is used 
during parsing. The parser can then generate appropriate error diagnostics 
about the erroneous construct that has been recognized in the input. 
Global Correction 
Ideally, we would like a compiler to make as few changes as possible in processing 
an incorrect input string. There are algorithms for choosing a minimal sequence 
of changes to obtain a globally least-cost correction. Given an incorrect input 
string x and grammar G, these algorithms will find a parse tree for a related 
string y, such that the number of insertions, deletions, and changes of tokens 
required to transform x into y is as small as possible. Unfortunately, these 
methods are in general too costly to implement in terms of time and space, so 
these techniques are currently only of theoretical interest. 
Do note that a closest correct program may not be what the programmer had 
in mind. Nevertheless, the notion of least-cost correction provides a yardstick 
for evaluating error-recovery techniques, and has been used for finding optimal 
replacement strings for phrase-level recovery. 
4.2. CONTEXT-FREE GRAMMARS 
4.2 Context-Free Grammars 
Grammars were introduced in Section 2.2 to systematically describe the syntax 
of programming language constructs like expressions and statements. Using 
a syntactic variable stmt to denote statements and variable expr to denote 
expressions, the production 
stmt -+ if ( expr ) stmt else stmt 
(4.4) 
specifies the structure of this form of conditional statement. Other productions 
then define precisely what an expr is and what else a stmt can be. 
This section reviews the definition of a context-free grammar and introduces 
terminology for talking about parsing. In particular, the notion of derivations 
is very helpful for discussing the order in which productions are applied during 
parsing. 
4.2.1 The Formal Definition of a Context-Free Grammar 
From Section 2.2, a context-free grammar (grammar for short) consists of ter- 
minals, nonterminals, a start symbol, and productions. 
1. Terminals are the basic symbols from which strings are formed. The term 
"token name" is a synonym for '"erminal" and frequently we will use the 
word "token" for terminal when it is clear that we are talking about just 
the token name. We assume that the terminals are the first components 
of the tokens output by the lexical analyzer. In (4.4), the terminals are 
the keywords if and else and the symbols "(" and ") ." 
2. Nonterminals are syntactic variables that denote sets of strings. In (4.4), 
stmt and expr are nonterminals. The sets of strings denoted by nontermi- 
nals help define the language generated by the grammar. Nonterminals 
impose a hierarchical structure on the language that is key to syntax 
analysis and translation. 
3. In a grammar, one nonterminal is distinguished as the start symbol, and 
the set of strings it denotes is the language generated by the grammar. 
Conventionally, the productions for the start symbol are listed first. 
4. The productions of a grammar specify the manner in which the termi- 
nals and nonterminals can be combined to form strings. Each production 
consists of: 
(a) A nonterminal called the head or left side of the production; this 
production defines some of the strings denoted by the head. 
(b) The symbol +. Sometimes : : = has been used in place of the arrow. 
(c) A body or right side consisting of zero or more terminals and non- 
terminals. The components of the body describe one way in which 
strings of the nonterminal at the head can be constructed. 
198 
CHAPTER 4. SYNTAX ANALYSIS 
Example 4.5 : The grammar in Fig. 4.2 defines simple arithmetic expressions. 
In this grammar, the terminal symbols are 
The nonterminal symbols are expression, term and factor, and expression is the 
start symbol 
expression 
expression 
expression 
term 
term 
term 
factor 
factor 
expression + term 
expression - term 
term 
term * factor 
term / factor 
factor 
( expression 1 
id 
Figure 4.2: Grammar for simple arithmetic expressions 
4.2.2 
Notational Convent ions 
To avoid always having to state that "these are the terminals," "these are the 
nontermiaals ," and so on, the following notational conventions for grammars 
will be used throughout the remainder of this book. 
1. These symbols are terminals: 
(a) Lowercase letters early in the alphabet, such as a, b, e. 
(b) Operator symbols such as +, r, and so on. 
(c) Punctuation symbols such as parentheses, comma, and so on. 
(d) The digits 0,1,. . . ,9. 
(e) Boldface strings such as id or if, each of which represents a single 
terminal symbol. 
2. These symbols are nonterminals: 
(a) Uppercase letters early in the alphabet, such as A, B, C. 
(b) The letter S, which, when it appears, is usually the start symbol. 
(c) Lowercase, italic names such as expr or stmt. 
(d) When discussing programming constructs, uppercase letters may be 
used to represent nonterminals for the constructs. For example, non- 
terminals for expressions, terms, and factors are often represented by 
E, T, and F, respectively. 
4.2. CONTEXT-FREE GRAMMARS 
199 
3. Uppercase letters late in the alphabet, such as X, Y, 2, represent grammar 
symbols; that is, either nonterminals or terminals. 
4. Lowercase letters late in the alphabet, chiefly u, v, . . . , x, represent (pos- 
sibly empty) strings of terminals. 
5. Lowercase Greek letters, a, ,O, y for example, represent (possibly empty) 
strings of grammar symbols. Thus, a generic production can be written 
as A + a, where A is the head and a the body. 
6. A set of productions A -+ al, A + a2, . . . , A -+ a k with a common head 
A (call them A-productions), may be written A + a1 / as I . . I ak. Call 
al, a2,. . . , a k the alternatives for A. 
7. Unless stated otherwise, the head of the first production is the start sym- 
bol. 
Example 4.6 : Using these conventions, the grammar of Example 4.5 can be 
rewritten concisely as 
E 
T 
F 
+ E + T ( E- T I T 
+ T * F I T / F I F -+ 
(E) 1 id 
The notational conventions tell us that E, T, and F are nonterminals, with E 
the start symbol. The remaining symbols are terminals. 
4.2.3 Derivations 
The construction of a parse tree can be made precise by taking a derivational 
view, in which productions are treated as rewriting rules. Beginning with the 
start symbol, each rewriting step replaces a nonterminal by the body of one of its 
productions. This derivational view corresponds to the top-down construction 
of a parse tree, but the precision afforded by derivations will be especially helpful 
when bottom-up parsing is discussed. As we shall see, bottom-up parsing is 
related to a class of derivations known as "rightmost" derivations, in which the 
rightmost nonterminal is rewritten at each step. 
For example, consider the following grammar, with a single nonterminal E, 
which adds a production E -+ - E to the grammar (4.3): 
The production E -+ - E signifies that if E denotes an expression, then - E 
must also denote an expression. The replacement of a single E by - E will be 
described by writing 
CHAPTER 4. SYNTAX ANALYSIS 
which is read, "E derives -E." The production E --+ ( E ) can be applied 
to replace any instance of E in any string of grammar symbols by (E), e.g., 
E * E + (E) * E or E * E + E * (E). We can take a single E and repeatedly 
apply productions in any order to get a sequence of replacements. For example, 
We call such a sequence of replacements a derivation of -(id) from E. This 
derivation provides a proof that the string -(id) is one particular instance of 
an expression. 
For a general definition of derivation, consider a nonterminal A in the middle 
of a sequence of grammar symbols, as in aAP, where a and ,O are arbitrary 
strings of grammar symbols. Suppose A -+ y is a production. Then, we write 
aAP =+- ayp. The symbol +- means, "derives in one step." When a sequence 
of derivation steps a1 
+ a2 
+ . + a, rewrites a1 to a,, we say a1 derives 
a,. 
Often, we wish to say, "derives in zero or more steps." For this purpose, 
we can use the symbol &- . Thus, 
1. a % a, for any string a, and 
2. If a& p and p+y, then a% y. 
+ 
Likewise, + means, "derives in one or more steps." 
If S % a, where S is the start symbol of a grammar G, we say that a is a 
sentential form of G. Note that a sentential form may contain both terminals 
and nonterminals, and may be empty. A sentence of G is a sentential form with 
no nonterminals. The language generated by a grammar is its set of sentences. 
Thus, a string of terminals w is in L(G), the language generated by G, if and 
only if w is a sentence of G (or S % w). A language that can be generated by 
a grammar is said to be a context-free language. If two grammars generate the 
same language, the grammars are said to be equivalent. 
The string -(id + id) is a sentence of grammar (4.7) because there is a 
Likewise, + means, "derives in one or more steps." 
If S % a, where S is the start symbol of a grammar G, we say that a is a 
sentential form of G. Note that a sentential form may contain both terminals 
and nonterminals, and may be empty. A sentence of G is a sentential form with 
no nonterminals. The language generated by a grammar is its set of sentences. 
Thus, a string of terminals w is in L(G), the language generated by G, if and 
only if w is a sentence of G (or S % w). A language that can be generated by 
a grammar is said to be a context-free language. If two grammars generate the 
same language, the grammars are said to be equivalent. 
The string -(id + id) is a sentence of grammar (4.7) because there is a 
derivation 
E S- -E S- -(E) + -(E + E) 3 -(id + E) + -(id + id) 
(4.8) 
The strings E, - E, - (E) , . . . , - (id + id) are all sentential forms of this gram- 
mar. We write & % - (id + id) to indicate that - (id + id) can be derived 
from E. 
At each step in a derivation, there are two choices to be made. We need 
to choose which nonterminal to replace, and having made this choice, we must 
pick a production with that nonterminal as head. For example, the following 
alternative derivation of -(id + id) differs from derivation (4.8) in the last two 
steps: 
4.2. CONTEXT-FREE GRAMMARS 
20 1 
Each nonterminal is replaced by the same body in the two derivations, but the 
order of replacements is different. 
To understand how parsers work, we shall consider derivations in which the 
nonterminal to be replaced at each step is chosen as follows: 
1. In lefimost derivations, the leftmost nonterminal in each sentential is al- 
replaced, we write a 
ways chosen. If a + p is a step in which the leftmost nonterminal in a is 
P. 
lm 
2. In rightmost derivations, the rightmost nonterminal is always chosen; we 
write a + p in this case. 
rm 
Derivation (4.8) is leftmost, so it can be rewritten as 
Note that (4.9) is a rightmost derivation. 
Using our notational conventions, every leftmost step can be written as 
wAy + wSy, where w consists of terminals only, A -+ 6 is the production 
lm 
applied, and y is a string of grammar symbols. To emphasize that a derives ,8 
by a leftrnost derivation, we write a % p. If S % a, then we say that a is a 
lm 
left-sentential form of the grammar at hand. 
lm 
Analogous definitions hold for rightmost derivations. Rightmost derivations 
are sometimes called canonical derivations. 
4.2.4 Parse Trees and Derivations 
A parse tree is a graphical representation of a derivation that filters out the 
order in which productions are applied to replace nonterminals. Each interior 
node of a parse tree represents the application of a production. The interior 
node is labeled with the  ont terminal A in the head of the production; the 
children of the node are labeled, from left to right, by the symbols in the body 
of the production by which this A was replaced during the derivation. 
For example, the parse tree for -(id + id) in Fig. 4.3, results from the 
derivation (4.8) as well as derivation (4.9). 
The leaves of a parse tree are labeled by nonterminals or terminals and, read 
from left to right, constitute a sentential form, called the yield or frontier of the 
tree. 
To see the relationship between derivations and parse trees, consider any 
derivation a1 .j a2 
+ . - . + a,, where a1 is a single nonterminal A. For each 
sentential form ai in the derivation, we can construct a parse tree whose yield 
is ai. The process is an induction on i. 
BASIS: The tree for a1 = A is a single node labeled A. 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.3: Parse tree for -(id + id) 
INDUCTION: Suppose we already have constructed a parse tree with yield 
ai-1 = XI X2 . . Xk (note that according to our notational conventions, each 
grammar symbol Xi is either a nonterminal or a terminal). Suppose ai is 
derived from ai-1 by replacing Xj, a nonterminal, by ,8 = Y1Y2 . Ym. That 
is, at the ith step of the derivation, production Xj -+ ,8 is applied to ai-1 to 
derive ai = XIXz -. -Xj-1,8Xj+l . exIE' 
To model this step of the derivation, find the jth leaf from the left in the 
current parse tree. This leaf is labeled Xj. Give this leaf m children, labeled 
Yl, Y2,. . . , Ym, from the left. As a special case, if m = 0, then ,8 = e, and we 
give the jth leaf one child labeled E. 
Example 4.10 : The sequence of parse trees constructed from the derivation 
(4.8) is shown in Fig. 4.4. In the first step of the derivation, E + -E. To 
model this step, add two children, labeled - and E, to the root E of the initial 
tree. The result is the second tree. 
In the second step of the derivation, - E + - (E). Consequently, add three 
children, labeled (, E, and ), to the leaf labeled E of the second tree, to 
obtain the third tree with yield -(E). Continuing in this fashion we obtain the 
complete parse tree as the sixth tree. 
Since a parse tree ignores variations in the order in which symbols in senten- 
tial forms are replaced, there is a many-to-one relationship between derivations 
and parse trees. For example, both derivations (4.8) and (4.9), are associated 
with the same final parse tree of Fig. 4.4. 
In what follows, we shall frequently parse by producing a leftmost or a 
rightmost derivation, since there is a one-to-one relationship between parse 
trees and either leftmost or rightmost derivations. Both leftmost and rightmost 
derivations pick a particular order for replacing symbols in sentential forms, so 
they too filter out variations in the order. It is not hard to show that every parse 
tree has associated with it a unique leftmost and a unique rightmost derivation. 
4.2. CONTEXT-FREE GRAMMARS 
Figure 4.4: Sequence of parse trees for derivation (4.8) 
4.2.5 
Ambiguity 
From Section 2.2.4, a grammar that produces more than one parse tree for some 
sentence is said to be ambiguous. Put another way, an ambiguous grammar is 
one that produces more than one leftmost derivation or more than one rightmost 
derivation for the same sentence. 
Example 4.11 : The arithmetic expression grammar (4.3) permits two distinct 
leftmost derivations for the sentence id + id * id: 
The corresponding parse trees appear in Fig. 4.5. 
Note that the parse tree of Fig. 4.5(a) reflects the commonly assumed prece- 
dence of + and *, while the tree of Fig. 4.5(b) does not. That is, it is customary 
to treat operator * as having higher precedence than +, corresponding to the 
fact that we would normally evaluate an expression like a + b * c as a + (b * c), 
rather than as (a + b) * c. 
For most parsers, it is desirable that the grammar be made unambiguous, 
for if it is not, we cannot uniquely determine which parse tree to select for a 
sentence. In other cases, it is convenient to use carefully chosen ambiguous 
grammars, together with disambiguating rules that "throw away" undesirable 
parse trees, leaving only one tree for each sentence. 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.5: Two parse trees for id+id*id 
4.2.6 
Verifying the Language Generated by a Grammar 
Although compiler designers rarely do so for a complete programming-language 
grammar, it is useful to be able to reason that a given set of productions gener- 
ates a particular language. Troublesome constructs can be studied by writing 
a concise, abstract grammar and studying the language that it generates. We 
shall construct such a grammar for conditional statements below. 
A proof that a grammar G generates a language L has two parts: show that 
every string generated by G is in L, and conversely that every string in L can 
indeed be generated by G. 
Example 4.12 : Consider the following grammar: 
It may not be initially apparent, but this simple grammar generates all 
strings of balanced parentheses, and only such strings. To see why, we shall 
show first that every sentence derivable from S is balanced, and then that every 
balanced string is derivable from S. To show that every sentence derivable from 
S is balanced, we use an inductive proof on the number of steps n in a derivation. 
BASIS: The basis is n = 1. The only string of terminals derivable from S in 
one step is the empty string, which surely is balanced. 
INDUCTION: Now assume that all derivations of fewer than n steps produce 
balanced sentences, and consider a leftmost derivation of exactly n steps. Such 
a derivation must be of the form 
The derivations of x and y from S take fewer than n steps, so by the inductive 
hypothesis x and y are balanced. Therefore, the string (x)y must be balanced. 
That is, it has an equal number of left and right parentheses, and every prefix 
has at least as many left parentheses as right. 
4.2. CONTEXT-FREE GRAMMARS 
205 
Having thus shown that any string derivable from S is balanced, we must 
next show that every balanced string is derivable from S. To do so, use induction 
on the length of a string. 
BASIS: If the string is of length 0, it must be E, which is balanced. 
INDUCTION: First, observe that every balanced string has even length. As- 
sume that every balanced string of length less than 2n is derivable from S, 
and consider a balanced string w of length 2n, n 2 1. Surely w begins with a 
left parenthesis. Let (x) be the shortest nonempty prefix of w having an equal 
number of left and right parentheses. Then w can be written as w = (x) y where 
both x and y are balanced. Since x and y are of length less than 2n, they are 
derivable from S by the inductive hypothesis. Thus, we can find a derivation 
of the form 
proving that w = (x)y is also derivable from S. 
4.2.7 Context-Free Grammars Versus Regular 
Expressions 
Before leaving this section on grammars and their properties, we establish that 
grammars are a more powerful notation than regular expressions. Every con- 
struct that can be described by a regular expression can be described by a gram- 
mar, but not vice-versa. Alternatively, every regular language is a context-free 
language, but not vice-versa. 
For example, the regular expression (alb)*abb and the grammar 
describe the same language, the set of strings of a's and b's ending in abb. 
We can construct mechanically a grammar to recognize the same language 
as a nondeterministic finite automaton (NFA). The grammar above was con- 
structed from the NFA in Fig. 3.24 using the following construction: 
1. For each state i of the NFA, create a nonterminal Ai. 
2. If state i has a transition to state j on input a, add the production Ai -+ 
aAj. If state i goes to state j on input E, add the production Ai --+ A,. 
3. If i is an accepting state, add Ai -+ e. 
4. If i is the start state, make Ai be the start symbol of the grammar. 
206 
CHAPTER 4. SYNTAX ANALYSIS 
On the other hand, the language L = {anbn I n > 1) with an equal number 
of a's and b's is a prototypical example of a language that can be described 
by a grammar but not by a regular expression. To see why, suppose L were 
the language defined by some regular expression. We could construct a DFA D 
with a finite number of states, say k, to accept L. Since D has only k states, for 
an input beginning with more than k a's, D must enter some state twice, say 
si, as in Fig. 4.6. Suppose that the path from si back to itself is labeled with 
a sequence ajdi. Since aib<s in the language, there must be a path labeled bi 
from si to an accepting state f. But, then there is also a path from the initial 
state so through si to f labeled ajbi, as shown in Fig. 4.6. Thus, D also accepts 
ajbi, which is not in the language, contradicting the assumption that L is the 
language accepted by D. 
path labeled aj-i 
... 
a ~ t h 
labeled a' 
... 
path labeled b" 
... 
Figure 4.6: DFA D accepting both ai bi and aj bi. 
Colloquially, we say that "finite automata cannot count ," meaning that 
a finite automaton cannot accept a language like {anbn I n > 1) that would 
require it to keep count of the number of a's before it sees the b's. Likewise, "a 
grammar can count two items but not three," as we shall see when we consider 
non-context-free language constructs in Section 4.3.5. 
4.2.8 Exercises for Section 4.2 
Exercise 4.2.1 : Consider the context-free grammar: 
and the string aa + a*. 
a) Give a leftmost derivation for the string. 
b) Give a rightmost derivation for the string. 
c) Give a parse tree for the string. 
! d) Is the grammar ambiguous or unambiguous? Justify your answer. 
! e) Describe the language generated by this grammar. 
Exercise 4.2.2 : Repeat Exercise 4.2.1 for each of the following grammas and 
strings: 
4.2. CONTEXT-FREE GRAMMARS 
b) S -+ + S S ( * S S I a with string + * aaa. 
! C) S -+ S ( S ) S ( E with string (00). -+ 
! e) S -+ (L) I a and L-+ L, S I S with string ((a,a),a,(a)). 
!! f) S 
a S b S I b S a S I E with string aabbab. 
! g) The following grammar for boolean expressions: 
bexpr 
bterm 
bfactor -+ -+ --+ 
bexpr or bterm 1 bterm 
bterm and bfactor 1 bfactor 
not bfactor 1 ( bexpr ) 1 true 1 false 
Exercise 4.2.3 : Design grammars for the following languages: 
a) The set of all strings of 0s and 1s such that every 0 is immediately followed 
by at least one 1. 
! b) The set of all strings of 0s and 1s that are palindromes; that is, the string 
reads the same backward as forward. 
! c) The set of all strings of 0s and 1s with an equal number of 0s and 1s. 
!! d) The set of all strings of 0s and 1s with an unequal number of 0s and 1s. 
! e) The set of all strings of 0s and 1s in which 011 does not appear as a 
substring. 
!! f) The set of all strings of 0s and 1s of the form xy, where x # y and x and 
y are of the same length. 
! Exercise 4.2.4 : There is an extended grammar notation in common use. In 
this notation, square and curly braces in production bodies are metasymbols 
(like -+ or 1) with the following meanings: 
i) Square braces around a grammar symbol or symbols denotes that these 
constructs are optional. Thus, production A -+ X [Y] Z has the same 
effect as the two productions A -+ X Y Z and A -+ X 2. 
ii) Curly braces around a grammar symbol or symbols says that these sym- 
bols may be repeated any number of times, including zero times. Thus, 
A -+ X {Y Z) has the same effect as the infinite sequence of productions 
A-+X,A-+XY Z,A-+XY ZY Z,andsoon. 
208 
CHAPTER 4. SYNTAX ANALYSIS 
Show that these two extensions do not add power to grammars; that is, any 
language that can be generated by a grammar with these extensions can be 
generated by a grammar without the extensioms. 
Exercise 4.2.5 : Use the braces described in Exercise 4.2.4 to simplify the 
following grammar for statement blocks and conditional statements: 
stmt -i 
I 
I 
if expr then stmt else stmt 
if stmt then stmt 
begin stmtList end 
stmtList -i stmt ; stmtLdst ( stmt 
! Exercise 4.2.6 : Extend the idea of Exercise 4.2.4 to allow any regular expres- 
sion of grammar symbols in the body of a production. Show that this extension 
does not allow grammars to define any new languages. 
! Exercise 4.2.7 : A grammar symbol X (terminal or nonterminal) is useless if 
there is no derivation of the form S $- wXy % wzy. That is, X can never 
appear in the derivation of any sentence. 
a) Give an algorithm to eliminate from a grammar all productions containing 
useless symbols. 
b) Apply your algorithm to the grammar: 
Exercise 4.2.8: The grammar in Fig. 4.7 generates declarations for a sin- 
gle numerical identifier; these declarations involve four different, independent 
properties of numbers. 
stmt 
optionList 
option 
mode 
scale 
precision 
base -+ -+ -+ -+ 
declare id optionList 
optionList option I E 
mode I scale 1 precision I base 
real 1 complex 
+ fixed I floating 
+ single I double 
+ binary ( decimal 
Figure 4.7: A grammar for multi-attribute declarations 
a) Generalize the grammar of Fig. 4.7 by allowing n options Ai, for some 
fixed n and for i = 1,2 . . . , n, where Ai can be either ai or bi. Your 
grammar should use only O(n) grammar symbols and have a total length 
of productions that is O(n) . 
4.3. WRITING A GRAMMAR 
209 
! b) The grammar of Fig. 4.7 and its generalization in part (a) allow declara- 
tions that are contradictory and/or redundant, such as: 
declare foo real fixed real floating 
We could insist that the syntax of the language forbid such declarations; 
that is, every declaration generated by the grammar has exactly one value 
for each of the n options. If we do, then for any fixed n there is only a finite 
number of legal declarations. The language of legal declarations thus has 
a grammar (and also a regular expression), as any finite language does. 
The obvious grammar, in which the start symbol has a production for 
every legal declaration has n! productions and a total production length 
of O(n x n!). You must do better: a total production length that is 
0 (nzn) . 
!! c) Show that any grammar for part (b) must have a total production length 
of at least 2". 
d) What does part (c) say about the feasibility of enforcing nonredundancy 
and noncontradiction among options in declarations via the syntax of the 
programming language? 
4.3 
Writing a Grammar 
Grammars are capable of describing most, but not all, of the syntax of pro- 
gramming languages. For instance, the requirement that identifiers be declared 
before they are used, cannot be described by a context-free grammar. Therefore, 
the sequences of tokens accepted by a parser form a superset of the program- 
ming language; subsequent phases of the compiler must analyze the output of 
the parser to ensure compliance with rules that are not checked by the parser. 
This section begins with a discussion of how to divide work between a lexical 
analyzer and a parser. We then consider several transformations that could be 
applied to get a grammar more suitable for parsing. One technique can elim- 
inate ambiguity in the grammar, and other techniques - left-recursion elimi- 
nation and left factoring - are useful for rewriting grammars so they become 
suitable for top-down parsing. We conclude this section by considering some 
programming language constructs that cannot be described by any grammar. 
4.3.1 
Lexical Versus Syntactic Analysis 
As we observed in Section 4.2.7, everything that can be described by a regular 
expression can also be described by a grammar. We may therefore reasonably 
ask: "Why use regular expressions to define the lexical syntax of a language?" 
There are several reasons. 
210 
CHAPTER 4. SYNTAX ANALYSIS 
1. Separating the syntactic structure of a language into lexical and non- 
lexical parts provides a convenient way of modularizing the front end of 
a compiler into two manageable-sized components. 
2. The lexical rules of a language are frequently quite simple, and to describe 
them we do not need a notation as powerful as grammars. 
3. Regular expressions generally provide a more concise and easier-to-under- 
stand notation for tokens than grammars. 
4. More efficient lexical analyzers can be constructed automatically from 
regular expressions than from arbitrary grammars. 
There are no firm guidelines as to what to put into the lexical rules, as op- 
posed to the syntactic rules. Regular expressions are most useful for describing 
the structure of constructs such as identifiers, constants, keywords, and white 
space. Grammars, on the other hand, are most useful for describing nested 
structures such as balanced parentheses, matching begin-end's, corresponding 
if-then-else's, and so on. These nested structures cannot be described by regular 
expressions. 
4.3.2 Eliminating Ambiguity 
Sometimes an ambiguous grammar can be rewritten to eliminate the ambiguity. 
As an example, we shall eliminate the ambiguity from the following "dangling- 
else" grammar: 
stmt 
+ if expr then stmt 
I 
( 
if expr then stmt else stmt 
other 
(4.14) 
Here "other" stands for any other statement. According to this grammar, the 
compound conditional statement 
if El then S1 else if E2 then S2 else S3 
/Ti\\ 
2% then stmt 
El 
S1 
,L.Ll 
epT(\\\ 
n then stmt else stmt 
if 
LLL 
E2 
5'2 
S3 
Figure 4.8: Parse tree for a conditional statement 
4.3. WRITING A GRAMMAR 
211 
has the parse tree shown in Fig. 4.8.' Grammar (4.14) is ambiguous since the 
string 
(4.15) 
if El then if E2 then S1 else S2 
has the two parse trees shown in Fig. 4.9. 
if 
,expr\ 
t~itmj~ll 
El 
if 
if 
A then ,stmt\ 
E2 
,expr, 
then ,stmt, 
else /stmt\ 
S1 
s2 
Figure 4.9: Two parse trees for an ambiguous sentence 
In all programming languages with conditional statements of this form, the 
first parse tree is preferred. The general rule is, "Match each else with the 
closest unmatched then." This disambiguating rule can theoretically be in- 
corporated directly into a grammar, but in practice it is rarely built into the 
productions. 
Example 4.16 : We can rewrite the dangling-else grammar (4.14) as the fol- 
lowing unambiguous grammar. The idea is that a statement appearing between 
a then and an else must be "matched" ; that is, the interior statement must 
not end with an unmatched or open then. A matched statement is either an 
if-then-else statement containing no open statements or it is any other kind 
of unconditional statement. Thus, we may use the grammar in Fig. 4.10. This 
grammar generates the same strings as the dangling-else grammar (4.14), but 
it allows only one parsing for string (4.15); namely, the one that associates each 
else with the closest previous unmatched then. [7 
 he subscripts on E and S are just to distinguish different occurrences of the same 
nonterminal, and do not imply distinct nonterminals. 
2 ~ e 
should note that C and its derivatives are included in this class. Even though the C 
family of languages do not use the keyword then, its role is played by the closing parenthesis 
for the condition that follows if. 
CHAPTER 4. SYNTAX ANALYSIS 
stmt 
+ matched-stmt 
( 
open-stmt 
matched-stmt + if expr then matched-stmt else matched-stmt 
1 
other 
open-stmt + if expr then stmt 
1 
if expr then matched-stmt else open-stmt 
Figure 4.10: Unambiguous grammar for if-then-else statements 
4.3.3 Elimination of Left Recursion 
A grammar is left recursive if it has a nonterminal A such that there is a 
+ 
derivation A * Aa for some string a. Top-down parsing methods cannot 
handle left-recursive grammars, so a transformation is needed to eliminate left 
recursion. In Section 2.4.5, we discussed immediate left recursion, where there 
is a production of the form A --+ Aa. Here, we study the general case. In 
Section 2.4.5, we showed how the left-recursive pair of productions A -+ Aa 1 ,fl 
could be replaced by the non-left-recursive productions: 
without changing the strings derivable from A. This rule by itself suffices for 
many grammars. 
Example 4.17 : The non-left-recursive expression grammar (4.2), repeated 
here, 
is obtained by eliminating immediate left recursion from the expression gram- 
mar (4.1). The left-recursive pair of productions E -+ E + T I T are replaced 
by E -+ T E' and E' -+ + T E' I c. The new productions for T and T' are 
obtained similarly by eliminating immediate left recursion. 
Immediate left recursion can be eliminated by the following technique, which 
works for any number of A-productions. First, group the productions as 
where no pi begins with an A. Then, replace the A-productions by 
4.3. WRITING A GRAMMAR 
The nonterminal A generates the same strings as before but is no longer left 
recursive. This procedure eliminates all left recursion from the A and A' pro- 
ductions (provided no ai is E), but it does not eliminate left recursion involving 
derivations of two or more steps. For example, consider the grammar 
Aa + Sda, but it is not 
The nonterminal S is left recursive because S 
immediately left recursive. 
Algorithm 4.19, below, systematically eliminates left recursion from a gram- 
mar. It is guaranteed to work if the grammar has no cycles (derivations of the 
+ 
form A + A) or 6-productions (productions of the form A -+ E). Cycles can be 
eliminated systematically from a grammar, as can E-productions (see Exercises 
4.4.6 and 4.4.7). 
Algorithm 4.19 : Eliminating left recursion. 
INPUT: Grammar G with no cycles or e-productions. 
OUTPUT: An equivalent grammar with no left recursion. 
METHOD: Apply the algorithm in Fig. 4.11 to G. Note that the resulting 
non-left-recursive grammar may have E-productions. 
1) 
2) 
3) 
4) 
> 
5 
6) 
7) 
arrange the nonterminals in some order A1, A2, . . . , A,. 
for ( each i from 1 to n ) { 
for ( each j from 1 to i - 1 ) { 
replace each production of the form Ai -+ Aj7 by the 
productions Ai -+ 617 I 627 1 -. I dk7, where 
Aj -+ dl 1 d2 1 . . . 1 dk are all current Aj-productions 
} 
eliminate the immediate left recursion among the Ai-productions 
1 
Figure 4.11: Algorithm to eliminate left recursion from a grammar 
The procedure in Fig. 4.11 works as follows. In the first iteration for i = 
1, the outer for-loop of lines (2) through (7) eliminates any immediate left 
recursion among A1-productions. Any remaining A1 productions of the form 
Al -+ Ala must therefore have 1 > 1. After the i - 1st iteration of the outer for- 
loop, all nonterminals Ale, where k < i, are "cleaned"; that is, any production 
Ak -+ Ala, must have 1 > k. As a result, on the ith iteration, the inner loop 
214 
CHAPTER 4. SYNTAX ANALYSIS 
of lines (3) through (5) progressively raises the lower limit in any production 
Ai -+ A,a, until we have m _> i. Then, eliminating immediate left recursion 
for the Ai productions at line (6) forces m to be greater than i. 
Example 4.20 : Let us apply Algorithm 4.19 to the grammar (4.18). Techni- 
cally, the algorithm is not guaranteed to work, because of the €-production, but 
in this case, the production A -+ c turns out to be harmless. 
We order the nonterminals S, A. There is no immediate left recursion 
among the S-productions, so nothing happens during the outer loop for i = 1. 
For i = 2, we substitute for S in A -+ S d to obtain the following A-productions. 
A-+Ac I Aad 1 bd 1 E 
Eliminating the immediate left recursion among these A-productions yields the 
following grammar. 
4.3.4 
Left Factoring 
a, 
Left factoring is a grammar transformation that is useful for producing a gram- 
mar suitable for predictive, or top-down, parsing. When the choice between 
two alternative A-productions is not clear, we may be able to rewrite the pro- 
ductions to defer the decision until enough of the input has been seen that we 
can make the right choice. 
For example, if we have the two productions 
stmt -+ 
I 
if expr then stmt else strnt 
if expr then stmt 
on seeing the input if, we cannot immediately tell which production to choose 
to expand stmt. In general, if A + apl I aP2 are two A-productions, and the 
input begins with a nonempty string derived from a, we do not know whether 
to expand A to aPl or ah. However, we may defer the decision by expanding 
A to aA'. Then, after seeing the input derived from a, we expand A' to PI or 
to P2. That is, left-factored, the original productions become 
Algorithm 4.2 1 : Left factoring a grammar. 
INPUT: Grammar G. 
OUTPUT: An equivalent left-factored grammar. 
4.3. WRITING A GRAMMAR 
215 
METHOD: For each nonterminal A, find the longest prefix a! common to two 
or more of its alternatives. If a! # E - i.e., there is a nontrivial common 
prefix - replace all of the A-productions A + up1 1 cupz 1 - - . / a!/?, 
I y, where 
y represents all alternatives that do not begin with a, by 
Here A' is a new nonterminal. Repeatedly apply this transformation until no 
two alternatives for a nonterminal have a common prefix. 
Example 4.22 : The following grammar abstracts the "dangling-else" prob- 
lem: 
Here, i, t, and e stand for if, then, and else; E and S stand for "conditional 
expression" and "statement ." Left-factored, this grammar becomes: 
Thus, we may expand S to iEtSS1 on input i, and wait until iEtS has been 
seen to decide whether to expand St to eS or to e. Of course, these grammars 
are both ambiguous, and on input e, it will not be clear which alternative for 
St should be chosen. Example 4.33 discusses a way out of this dilemma. 
4.3.5 Non-Context-Free Language Constructs 
A few syntactic constructs found in typical programming languages cannot be 
specified using grammars alone. Here, we consider two of these constructs, 
using simple abstract languages to illustrate the difficulties. 
Example 4.25 : The language in this example abstracts the problem of check- 
ing that identifiers are declared before they are used in a program. The language 
consists of strings of the form wcw, where the first w represents the declaration 
of an identifier w, c represents an intervening program fragment, and the second 
w represents the use of the identifier. 
The abstract language is L1 = {wcw I w is in (alb)*). L1 consists of 
all words composed of a repeated string of a's and b's separated by c, such 
as aabcaab. While it is beyond the scope of this book to prove it, the non- 
context-freedom of L1 directly implies the non-context-freedom of programming 
languages like C and Java, which require declaration of identifiers before their 
use and which allow identifiers of arbitrary length. 
For this reason, a grammar for C or Java does not distinguish among identi- 
fiers that are different character strings. Instead, all identifiers are represented 
216 
CHAPTER 4. SYNTAX ANALYSIS 
by a token such as id in the grammar. In a compiler for such a language, 
the semantic-analysis phase checks that identifiers are declared before they are 
used. 
Example 4.26 : The non-context-free language in this example abstracts the 
problem of checking that the number of formal parameters in the declaration of a 
function agrees with the number of actual parameters in a use of the function. 
The language consists of strings of the form anbmcndm. (Recall an means a 
written n times.) Here an and bm could represent the formal-parameter lists of 
two functions declared to have n and rn arguments, respectively, while cn and 
dm represent the actual-parameter lists in calls to these two functions. 
The abstract language is Lz = {anbmcndm I n > 1 and m > I). That is, La 
consists of strings in the language generated by the regular expression a*b*c*d" 
such that the number of a's and c's are equal and the number of b's and d's are 
equal. This language is not context free. 
Again, the typical syntax of function declarations and uses does not concern 
itself with counting the number of parameters. For example, a function call in 
C-like language might be specified by 
stmt 
expr-list 
+ id ( expr-list ) 
+ expr-list , expr 
I 
expr 
with suitable productions for expr. Checking that the number of parameters in 
a call is correct is usually done during the semantic-analysis phase. 
4.3.6 Exercises for Section 4.3 
Exercise 4.3.1 : The following is a grammar for regular expressions over sym- 
bols a and b only, using + in place of 1 for union, to avoid conflict with the use 
of vertical bar as a metasymbol in grammars: 
rexpr + rterm ( rterm 
rterm 
rexpr 
rfactor 
rprimary 
a) Left factor this grammar. -+ -+ 
rterm rfactor I rfactor 
+ rfactor * 1 rprirnary 
+ a 1 b 
b) Does left factoring make the grammar suitable for top-down parsing? 
c) In addition to left factoring, eliminate left recursion from the original 
grammar. 
d) Is the resulting grammar suitable for top-down parsing? 
Exercise 4.3.2 : Repeat Exercise 4.3.1 on the following grammars: 
4.4. TOP-DO WN PARSING 
a) The grammar of Exercise 4.2.1. 
b) The grammar of Exercise 4.2.2(a). 
c) The grammar of Exercise 4.2.2(c). 
d) The grammar of Exercise 4.2.2(e). 
e) The grammar of Exercise 4.2.2(g). 
! Exercise 4.3.3 : The following grammar is proposed to remove the "dangling- 
else ambiguity" discussed in Section 4.3.2: 
stmt 
+ if expr then stmt 
I 
matchedstmt 
matchedstmt + if expr then matchedstmt else stmt 
1 
other 
Show that this grammar is still ambiguous. 
4.4 
Top-Down Parsing 
Top-down parsing can be viewed as the problem of constructing a parse tree for 
the input string, starting from the root and creating the nodes of the parse tree 
in preorder (depth-first, as discussed in Section 2.3.4). Equivalently, top-down 
parsing can be viewed as finding a leftmost derivation for an input string. 
Example 4.27 : The sequence of parse trees in Fig. 4.12 for the input id+id*id 
is a top-down parse according to grammar (4.2), repeated here: 
E 
E' 
+ TE' -+ 
+TE1( € 
T + FT' 
T' -+ 
*FTI I€ 
F + (E) ( id 
This sequence of trees corresponds to a leftmost derivation of the input. 
At each step of a top-down parse, the key problem is that of determining 
the production to be applied for a nonterminal, say A. Once an A-production 
is chosen, the rest of the parsing process consists of "matching7' the terminal 
symbols in the production body with the input string. 
The section begins with a general form of top-down parsing, called recursive- 
descent parsing, which may require backtracking to find the correct A-produc- 
tion to be applied. Section 2.4.2 introduced predictive parsing, a special case of 
recursive-descent parsing, where no backtracking is required. Predictive parsing 
chooses the correct A-production by looking ahead at the input a fixed number 
of symbols, typically we may look only at one (that is, the next input symbol). 
218 
CHAPTER 4. SYNTAX ANALYSIS 
Figure 4.12: Top-down parse for id + id * id 
For example, consider the top-down parse in Fig. 4.12, which constructs 
a tree with two nodes labeled El. At the first E' node (in preorder), the 
production E' -+ +TE' is chosen; at the second E' node, the production E' -+ t 
is chosen. A predictive parser can choose between El-productions by looking 
at the next input symbol. 
The class of grammars for which we can construct predictive parsers looking 
k symbols ahead in the input is sometimes called the LL(k) class. We discuss the 
LL(1) class in Section 4.4.3, but introduce certain computations, called FIRST 
and FOLLOW, in a preliminary Section 4.4.2. From the FIRST and FOLLOW 
sets for a grammar, we shall construct "predictive parsing tables," which make 
explicit the choice of production during top-down parsing. These sets are also 
useful during bottom-up parsing, 
In Section 4.4.4 we give a nonrecursive parsing algorithm that maintains 
a stack explicitly, rather than implicitly via recursive calls. Finally, in Sec- 
tion 4.4.5 we discuss error recovery during top-down parsing. 
4.4. TOP-DOWN PARSING 
4.4.1 Recursive-Descent Parsing 
void A() { 
1) 
2) 
3 
4) 
5 
6) 
7) 
Choose an A-production, A + XI X2 . . . Xk; 
for ( i=l tok) { 
if ( Xi is a nonterminal ) 
call procedure Xi () ; 
else if ( Xi equals the current input symbol a ) 
advance the input to the next symbol; 
else /* an error has occurred */; 
I 
1 
Figure 4.13: A typical procedure for a nonterminal in a top-down parser 
A recursive-descent parsing program consists of a set of procedures, one for each 
nonterminal. Execution begins with the procedure for the start symbol, which 
halts and announces success if its procedure body scans the entire input string. 
Pseudocode for a typical nonterminal appears in Fig. 4.13. Note that this 
pseudocode is nondeterministic, since it begins by choosing the A-production 
to apply in a manner that is not specified. 
General recursive-descent may require backtracking; that is, it may require 
repeated scans over the input. However, backtracking is rarely needed to parse 
programming language constructs, so backtracking parsers are not seen fre- 
quently. Even for situations like natural language parsing, backtracking is not 
very efficient, and tabular methods such as the dynamic programming algo- 
rithm of Exercise 4.4.9 or the method of Earley (see the bibliographic notes) 
are preferred. 
To allow backtracking, the code of Fig. 4.13 needs to be modified. First, we 
cannot choose a unique A-production at line (I), so we must try each of several 
productions in some order. Then, failure at line (7) is not ultimate failure, but 
suggests only that we need to return to line (1) and try another A-production. 
Only if there are no more A-productions to try do we declare that an input 
error has been found. In order to try another A-production, we need to be able 
to reset the input pointer to where it was when we first reached line (1). Thus, 
a local variable is needed to store this input pointer for future use. 
Example 4.29 : Consider the grammar 
To construct a parse tree top-down for the input string w = cad, begin with a 
tree consisting of a single node labeled S, and the input pointer pointing to c, 
the first symbol of w. S has only one production, so we use it to expand S and 
220 
CHAPTER 4. SYNTAX ANALYSIS 
obtain the tree of Fig. 4.14(a). The leftmost leaf, labeled c, matches the first 
symbol of input w, so we advance the input pointer to a, the second symbol of 
w, and consider the next leaf, labeled A. 
Figure 4.14: Steps in a top-down parse 
Now, we expand A using the first alternative A -+ a b to obtain the tree of 
Fig. 4.14(b). We have a match for the second input symbol, a, so we advance 
the input pointer to d, the third input symbol, and compare d against the next 
leaf, labeled b. Since b does not match d, we report failure and go back to A to 
see whether there is another alternative for A that has not been tried, but that 
might produce a match. 
In going back to A, we must reset the input pointer to position 2, the 
position it had when we first came to A, which means that the procedure for A 
must store the input pointer in a local variable. 
The second alternative for A produces the tree of Fig. 4.14(c). The leaf 
a matches the second symbol of w and the leaf d matches the third symbol. 
Since we have produced a parse tree for w, we halt and announce successful 
completion of parsing. El 
A left-recursive grammar can cause a recursive-descent parser, even one 
with backtracking, to go into an infinite loop. That is, when we try to expand 
a nonterminal A, we may eventually find ourselves again trying to expand A 
without having consumed any input. 
4.4.2 FIRST and FOLLOW 
The construction of both top-down and bottom-up parsers is aided by two 
functions, FIRST and FOLLOW, associated with a grammar G. During top- 
down parsing, FIRST and FOLLOW allow us to choose which production to 
apply, based on the next input symbol. During panic-mode error recovery, sets 
of tokens produced by FOLLOW can be used as synchronizing tokens. 
Define FIRST(&), where a is any string of grammar symbols, to be the set 
of terminals that begin strings derivedPom a. If a % 6, then E is also in 
FIRST@). For example, in Fig. 4.15, A + cy, so c is in FIRST(A). 
For a preview of how FIRST can be used during predictive parsing, consider 
two A-productions A + a / P, where FIRST(&) and FIRST@) are disjoint sets. 
We can then choose between these A-productions by looking at the next inp